{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac83f81-1cda-4ac2-a3f7-c774d87672fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "    Ans: The mathematical formula for a linear SVM can be expressed as follows: y = (W^T Â· x + b), where y is the predicted class label, w is the weight vector, \n",
    "         x is the input vector, b is the bias term, and sign is the sign function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067561b4-6563-4421-ae2b-1b8df0151cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "    Ans: The objective function of a linear SVM is to find the hyperplane that maximizes the margin between the two classes. This is achieved by minimizing the sum of the \n",
    "         squared norms of the weight vector subject to the constraint that each training example is classified correctly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff970e9e-0718-484c-b3a3-25a176f05258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q3. What is the kernel trick in SVM?\n",
    "\n",
    "    Ans: The kernel trick in SVM is a method of transforming the input features of a linear SVM into a higher-dimensional space without actually computing the coordinates of \n",
    "         the data in that space. This allows non-linear decision boundaries to be learned, making SVMs more powerful and flexible for classification tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6af09-4ae6-4880-b6e7-e5519a16d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q4. What is the role of support vectors in SVM Explain with example.\n",
    "\n",
    "    Ans: Support vectors are the training examples that lie closest to the decision boundary in an SVM. They are the critical points that define the location of the decision \n",
    "         boundary and are used to make predictions for new examples. The optimal decision boundary is determined by these support vectors, and examples that are not support \n",
    "         vectors do not contribute to the final solution. For example, in a binary classification task, there may be only a few support vectors that lie on or near the decision \n",
    "         boundary, while the rest of the training examples are far away from the boundary and do not affect the final solution.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80477de1-5f5f-4e75-9967-c66b128df2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?\n",
    "\n",
    "    Ans: 1. Hyperplane: A hyperplane in SVM is a decision boundary that separates the two classes. In a 2D feature space, a hyperplane is a straight line, and in a 3D feature \n",
    "                space, it is a flat plane. The hyperplane is defined by a weight vector and a bias term, and the objective of SVM is to find the hyperplane that maximizes the \n",
    "                margin between the classes.\n",
    "            Example: Suppose we have a dataset with two classes, represented by blue and red circles, and two features, x and y. We want to find the hyperplane that separates \n",
    "                the two classes.\n",
    "         2. Marginal plane: The marginal plane in SVM is the plane that is equidistant from the positive and negative support vectors. It is defined by the sum of the weight \n",
    "                 vector and the bias term. The distance between the marginal plane and the hyperplane is the margin, and the objective of SVM is to maximize this margin. \n",
    "            Example: Suppose we have a dataset with two classes, represented by blue and red circles, and two features, x and y. We have found the hyperplane that separates \n",
    "                the two classes and the positive and negative support vectors.\n",
    "         3. Soft margin: In some cases, the dataset may not be perfectly separable, and we need to allow for some misclassifications in order to find a good decision boundary. \n",
    "                 This is called a soft margin. The objective of SVM with a soft margin is to find a decision boundary that minimizes the number of misclassifications while \n",
    "                 still maximizing the margin.\n",
    "            Example: Suppose we have a dataset with two classes, represented by blue and red circles, and two features, x and y. We want to find a decision boundary that \n",
    "                 minimizes the number of misclassifications while still maximizing the margin.\n",
    "         4. Hard margin: If the dataset is perfectly separable, we can use a hard margin to find the decision boundary. A hard margin SVM tries to find the decision boundary \n",
    "                 that perfectly separates the two classes with no misclassifications.\n",
    "            Example: Suppose we have a dataset with two classes, represented by blue and red circles, and two features, x and y. We want to find a decision boundary that\n",
    "                 perfectly separates the two classes with no misclassifications.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "229ffad4-75ef-41ce-acbd-f038d09d3191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[16  0  0]\n",
      " [ 0 12  0]\n",
      " [ 0  0 17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        16\n",
      "  versicolor       1.00      1.00      1.00        12\n",
      "   virginica       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "{'C': 1, 'gamma': 1, 'kernel': 'linear'}\n",
      "0.9714285714285715\n",
      "1.0\n",
      "[[16  0  0]\n",
      " [ 0 12  0]\n",
      " [ 0  0 17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        16\n",
      "  versicolor       1.00      1.00      1.00        12\n",
      "   virginica       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Q6. SVM Implementation through Iris dataset.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "X = iris.iloc[:,:-1]\n",
    "y = iris.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=69)\n",
    "\n",
    "classifier = SVC(kernel='linear')\n",
    "classifier.fit(X_train,y_train)\n",
    "y_p = classifier.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_p))\n",
    "print(confusion_matrix(y_test, y_p))\n",
    "print(classification_report(y_test, y_p))\n",
    "\n",
    "p = {\n",
    "    'C':[0.1,1,10,100,1000],\n",
    "    'gamma':[1,0.1,0.01,0.001,0.0001],\n",
    "    'kernel':['linear']\n",
    "}\n",
    "\n",
    "g = GridSearchCV(SVC(), param_grid=p, cv=5)\n",
    "g.fit(X_train, y_train)\n",
    "print(g.best_params_)\n",
    "print(g.best_score_)\n",
    "y_p2 = g.predict(X_test)\n",
    "print(accuracy_score(y_test, y_p2))\n",
    "print(confusion_matrix(y_test, y_p2))\n",
    "print(classification_report(y_test, y_p2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
